{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Upload Housing Price Data\n",
    "\n",
    "Run the cells below to generate daily housing price data pulled from the Boston housing price dataset and upload it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role, LinearLearnerPredictor\n",
    "from datetime import date, datetime, timedelta\n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import boto3\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "def split_data_by_days(data, num_days):\n",
    "    num_rows = len(data)\n",
    "    split_data = zip(*[iter(data)] * int(num_rows / num_days))\n",
    "    return list(split_data)\n",
    "\n",
    "def write_to_csv(filename, data):\n",
    "    if not filename.endswith('.csv'):\n",
    "        filename = '{}.csv'.format(filename)\n",
    "    with open(filename, 'w', newline='\\n') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_days = 3 # number of days to split housing prices data into.\n",
    "model_name = 'LinearLearner-HomePrices' # If you modified the ModelPrefix CloudFormation template change this to the value you modified it to be.\n",
    "\n",
    "bucket = '<NAME OF YOUR BUCKET HERE>' # Set this to the name of bucket created by CloudFormation template. Can be found in the output of the template.\n",
    "prefix = 'data/{}/train'.format(model_name)\n",
    "\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "boston = load_boston()\n",
    "target = boston.target\n",
    "data = [np.ndarray.tolist(row) for row in boston.data[:, :]]\n",
    "\n",
    "# Add target value as first column as expected by training algorithm\n",
    "training_set = [[row[0]] + row[1] for row in zip(target, data)]\n",
    "\n",
    "# Split data into seperate datasets for each day\n",
    "train_by_day = split_data_by_days(training_set, num_days)\n",
    "\n",
    "# Upload split datasets to S3\n",
    "for day in range(num_days):\n",
    "    current_date = date.today() - timedelta(day)\n",
    "    key = '{}.csv'.format(current_date)\n",
    "    write_to_csv(key, train_by_day[day])\n",
    "    s3_uri = 's3://{}/{}/{}'.format(bucket, prefix, key)\n",
    "    print('Uploading {} to {}'.format(key, s3_uri))\n",
    "    boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, key)).upload_file(key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate JSON For Testing Execution\n",
    "\n",
    "Run the cell below then copy/paste the output JSON into the input of your Step Functions state machine from the Step Functions console to test the pipeline. This JSON is identical to the JSON that CloudWatch Events will send to Step Functions when triggering an execution of your state machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_json = {\n",
    "  \"version\": \"0\",\n",
    "  \"id\": \"89d1a02d-5ec7-412e-82f5-13505f849b41\",\n",
    "  \"detail-type\": \"Scheduled Event\",\n",
    "  \"source\": \"aws.events\",\n",
    "  \"account\": \"123456789012\",\n",
    "  \"time\": datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "  \"region\": \"us-east-1\",\n",
    "  \"resources\": [\n",
    "    \"arn:aws:events:us-east-1:123456789012:rule/SampleRule\"\n",
    "  ],\n",
    "  \"detail\": {}\n",
    "}\n",
    "\n",
    "print(json.dumps(test_json))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leverage Model Endpoint For Inference\n",
    "\n",
    "Run the cells below to create a predictor object for the latest model deployed to the SageMaker endpoint and use it to perform inference on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the predictor object\n",
    "predictor = LinearLearnerPredictor(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features for a house to infer the price of\n",
    "house1 = [0.04741, 0.0, 11.93, 0.0, 0.573, 6.03, 80.8, 2.505, 1.0, 273.0, 21.0, 396.9, 7.88]\n",
    "\n",
    "# Convert the data to float32 format and reshape it to an 1x13 dimensional numpy array\n",
    "# as this is the format and shape the predictor expects\n",
    "house1 = np.asarray(house1).astype('float32').reshape(1,13)\n",
    "\n",
    "# Predict medv house price\n",
    "predictor.predict(house1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on multiple houses at once\n",
    "\n",
    "house2 = [0.10959, 0.0, 11.93, 0.0, 0.573, 6.794, 89.3, 2.3889, 1.0, 273.0, 21.0, 393.45, 6.48]\n",
    "house2 = np.asarray(house2).astype('float32').reshape(1,13)\n",
    "\n",
    "two_houses = np.append(house1, house2, axis=0)\n",
    "\n",
    "predictor.predict(two_houses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
